{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from pathlib import Path\n","import requests\n","\n","DATA_PATH = Path(\"data\")\n","PATH = DATA_PATH / \"mnist\"\n","\n","PATH.mkdir(parents=True, exist_ok=True)\n","\n","URL = \"https://github.com/pytorch/tutorials/raw/master/_static/\"\n","FILENAME = \"mnist.pkl.gz\"\n","\n","if not (PATH / FILENAME).exists():\n","        content = requests.get(URL + FILENAME).content\n","        (PATH / FILENAME).open(\"wb\").write(content)\n","    \n","    "]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import pickle\n","import gzip\n","\n","with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n","        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["(50000, 784)\n"]},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/svg+xml":"<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 251.565 248.518125\" width=\"251.565pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-04-15T17:51:00.033765</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 248.518125 \nL 251.565 248.518125 \nL 251.565 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 26.925 224.64 \nL 244.365 224.64 \nL 244.365 7.2 \nL 26.925 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p7a000215b6)\">\n    <image height=\"218\" id=\"image1ac4e1b20a\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"26.925\" xlink:href=\"data:image/png;base64,\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\" y=\"-6.64\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m2cd23762fb\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.807857\" xlink:href=\"#m2cd23762fb\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(27.626607 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"69.636429\" xlink:href=\"#m2cd23762fb\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 5 -->\n      <g transform=\"translate(66.455179 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" id=\"DejaVuSans-35\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"108.465\" xlink:href=\"#m2cd23762fb\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 10 -->\n      <g transform=\"translate(102.1025 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"147.293571\" xlink:href=\"#m2cd23762fb\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 15 -->\n      <g transform=\"translate(140.931071 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"186.122143\" xlink:href=\"#m2cd23762fb\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 20 -->\n      <g transform=\"translate(179.759643 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"224.950714\" xlink:href=\"#m2cd23762fb\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 25 -->\n      <g transform=\"translate(218.588214 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"mbcc2b2b7e9\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mbcc2b2b7e9\" y=\"11.082857\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0 -->\n      <g transform=\"translate(13.5625 14.882076)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mbcc2b2b7e9\" y=\"49.911429\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 5 -->\n      <g transform=\"translate(13.5625 53.710647)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mbcc2b2b7e9\" y=\"88.74\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 10 -->\n      <g transform=\"translate(7.2 92.539219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mbcc2b2b7e9\" y=\"127.568571\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 15 -->\n      <g transform=\"translate(7.2 131.36779)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mbcc2b2b7e9\" y=\"166.397143\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 20 -->\n      <g transform=\"translate(7.2 170.196362)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mbcc2b2b7e9\" y=\"205.225714\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 25 -->\n      <g transform=\"translate(7.2 209.024933)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 26.925 224.64 \nL 26.925 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 244.365 224.64 \nL 244.365 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 26.925 224.64 \nL 244.365 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 26.925 7.2 \nL 244.365 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p7a000215b6\">\n   <rect height=\"217.44\" width=\"217.44\" x=\"26.925\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n","image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["from matplotlib import pyplot\n","import numpy as np\n","\n","pyplot.imshow(x_train[0].reshape((28,28)), cmap=\"gray\")\n","print(x_train.shape)\n","\n",""]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]]) tensor([5, 0, 4,  ..., 8, 4, 8])\ntorch.Size([50000, 784])\ntensor(0) tensor(9)\n"]}],"source":["import torch\n","\n","x_train, y_train, x_valid, y_valid = map(\n","    torch.tensor, (x_train, y_train, x_valid, y_valid)\n",")\n","n, c = x_train.shape\n","x_train, x_train.shape, y_train.min(), y_train.max()\n","print(x_train, y_train)\n","print(x_train.shape)\n","print(y_train.min(), y_train.max())\n",""]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["### neural network from scratch\n","import math\n","\n","weights = torch.randn(784, 10) / math.sqrt(784)\n","weights.requires_grad_()\n","bias = torch.zeros(10, requires_grad=True)\n","\n","\n","\n",""]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["def log_softmax(x):\n","    return x -x.exp().sum(-1).log().unsqueeze(-1)\n","\n","def model(xb):\n","    return log_softmax(xb @ weights + bias)\n",""]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([-2.9654, -2.0238, -2.0757, -1.8757, -2.0869, -2.2412, -2.4475, -2.8783,\n        -2.4981, -2.4983], grad_fn=<SelectBackward>) torch.Size([64, 10])\n"]}],"source":["bs = 64\n","xb = x_train[0:64]\n","preds = model(xb)\n","preds[0], preds.shape\n","print(preds[0], preds.shape)\n",""]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(2.3484, grad_fn=<NegBackward>)\n"]}],"source":["def nll(input, target):\n","    return -input[range(target.shape[0]), target].mean(-1)\n","\n","loss_func = nll\n","\n","yb= y_train[0:bs]\n","print(loss_func(preds, yb))"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(0.1094)\n"]}],"source":["def accuracy(out, yb):\n","    preds = torch.argmax(out, dim=1)\n","    return (preds == yb).float().mean()\n","\n","print(accuracy(preds, yb))"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["from IPython.core.debugger import set_trace"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"output_type":"stream","name":"stderr","text":["/home/gyasis/miniconda3/envs/autopytorch/lib/python3.9/site-packages/torch/autograd/__init__.py:145: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n  Variable._execution_engine.run_backward(\n"]}],"source":["lr = 0.5\n","epochs = 2 \n","\n","for epoch in range(epochs):\n","    for i in range((n-1) // bs +1):\n","        start_i = i * bs\n","        end_i = start_i + bs\n","        xb = x_train[start_i:end_i]\n","        yb = y_train[start_i:end_i]\n","        pred = model(xb)\n","        loss = loss_func(pred, yb)\n","        \n","        loss.backward()\n","        with torch.no_grad():\n","            weights -= weights.grad * lr\n","            bias -= bias.grad * lr\n","            weights.grad.zero_()\n","            bias.grad.zero_()\n","            "]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(0.0807, grad_fn=<NegBackward>) tensor(1.)\n"]}],"source":["print(loss_func(model(xb), yb), accuracy(model(xb), yb))"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(0.0807, grad_fn=<NllLossBackward>) tensor(1.)\n"]}],"source":["# Now with torch.nn\n","\n","import torch.nn.functional as F\n","\n","loss_func = F.cross_entropy\n","\n","def model(xb):\n","    return xb @ weights + bias\n","\n","print(loss_func(model(xb), yb), accuracy(model(xb), yb))"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["    \n","from torch import nn\n","\n","class Mnist_Logistic(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.weights = nn.Parameter(torch.randn(784, 10) / math.sqrt(784))\n","        self.bias = nn.Parameter(torch.zeros(10))\n","\n","    def forward(self, xb):\n","        return xb @ self.weights + self.bias    "]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["model = Mnist_Logistic()"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(2.5188, grad_fn=<NllLossBackward>)\n"]}],"source":["print(loss_func(model(xb), yb))"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["# previous loop \n","\n","\n","# for epoch in range(epochs):\n","#     for i in range((n-1) // bs +1):\n","#         start_i = i * bs\n","#         end_i = start_i + bs\n","#         xb = x_train[start_i:end_i]\n","#         yb = y_train[start_i:end_i]\n","#         pred = model(xb)\n","#         loss = loss_func(pred, yb)\n","        \n","#         loss.backward()\n","#         with torch.no_grad():\n","#             weights -= weights.grad * lr\n","#             bias -= bias.grad * lr\n","#             weights.grad.zero_()\n","#             bias.grad.zero_()\n","\n","\n","def fit():\n","    for epoch in range(epochs):\n","        for i in range((n-1) // bs +1):\n","            start_i = i * bs\n","            end_i = start_i + bs\n","            xb = x_train[start_i:end_i]\n","            yb = y_train[start_i:end_i]\n","            pred = model(xb)\n","            loss = loss_func(pred, yb)\n","            \n","            loss.backward()\n","            with torch.no_grad():\n","                for p in model.parameters():\n","                    p -= p.grad * lr\n","                model.zero_grad()    \n","            \n","fit()"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(0.0830, grad_fn=<NllLossBackward>)\n"]}],"source":["print(loss_func(model(xb), yb))"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["class Mnist_Logistic(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.lin = nn.Linear(784,10)\n","        \n","    def forward(self, xb):\n","        return self.lin(xb)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(2.3778, grad_fn=<NllLossBackward>)\n"]}],"source":["model = Mnist_Logistic()\n","print(loss_func(model(xb), yb))"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(0.0804, grad_fn=<NllLossBackward>)\n"]}],"source":["fit()\n","print(loss_func(model(xb),yb))"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(2.3447, grad_fn=<NllLossBackward>)\n","tensor(0.0817, grad_fn=<NllLossBackward>)\n"]}],"source":["from torch import optim\n","\n","def get_model():\n","    model = Mnist_Logistic()\n","    return model, optim.SGD(model.parameters(), lr=lr)\n","\n","model, opt  = get_model()\n","print(loss_func(model(xb), yb))\n","\n","for epoch in range(epochs):\n","    for i in range((n-1) // bs + 1):\n","        start_i = i * bs \n","        end_i = start_i + bs \n","        xb = x_train[start_i:end_i]\n","        yb = y_train[start_i:end_i]\n","        pred = model(xb)\n","        loss = loss_func(pred, yb)\n","        \n","        loss.backward()\n","        opt.step()\n","        opt.zero_grad()\n","        \n","print(loss_func(model(xb), yb))        "]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["from torch.utils.data import TensorDataset\n","\n","train_ds = TensorDataset(x_train, y_train)"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(0.0822, grad_fn=<NllLossBackward>)\n"]}],"source":["model, opt = get_model()\n","\n","for epoch in range(epochs):\n","    for i in range((n - 1) // bs + 1):\n","        xb, yb =train_ds[i * bs: i * bs + bs]\n","        pred = model(xb)\n","        loss = loss_func(pred, yb)\n","        loss.backward()\n","        opt.step()\n","        opt.zero_grad()\n","        \n","        \n","print(loss_func(model(xb), yb))        \n","        \n","              "]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(0.0804, grad_fn=<NllLossBackward>)\n"]}],"source":["from torch.utils.data import DataLoader\n","\n","train_dl = DataLoader(train_ds, batch_size=bs)\n","\n","for xb, yb in train_dl:\n","    pred = model(xb)\n","    \n","model, opt = get_model()\n","for epoch in range(epochs):\n","    for xb, yb in train_dl:\n","        pred = model(xb)\n","        loss = loss_func(pred, yb)\n","        \n","        loss.backward()\n","        opt.step()\n","        opt.zero_grad()\n","        \n","print(loss_func(model(xb), yb))"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["train_ds = TensorDataset(x_train, y_train)\n","train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n","\n","valid_ds = TensorDataset(x_valid, y_valid)\n","valid_dl = DataLoader(valid_ds, batch_size=bs * 2)\n",""]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["0 tensor(0.3029)\n","1 tensor(0.2969)\n"]}],"source":["model, opt = get_model()\n","\n","for epoch in range(epochs):\n","    model.train()\n","    for xb, yb in train_dl:\n","        pred = model(xb)\n","        loss = loss_func(pred, yb)\n","        \n","        loss.backward()\n","        opt.step()\n","        opt.zero_grad()\n","        \n","    model.eval()\n","    with torch.no_grad():\n","        valid_loss = sum(loss_func(model(xb), yb) for xb, yb in valid_dl)\n","    print(epoch, valid_loss / len(valid_dl))       "]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["def loss_batch(model, loss_func, xb, yb, opt=None):\n","    loss = loss_func(model(xb), yb)\n","    \n","    if opt is not None:\n","        loss.backward()\n","        opt.step(  )\n","        opt.zero_grad()\n","    return loss.item(), len(xb)    "]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["import numpy as np\n","\n","def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n","    for epoch in range(epochs):\n","        model.train()\n","        for xb, yb in train_dl:\n","            loss_batch(model, loss_func, xb, yb, opt)\n","            \n","        model.eval()\n","        with torch.no_grad():\n","            losses, nums = zip(\n","                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]\n","            )\n","        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n","        \n","        print(epoch, val_loss)        \n","        "]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["def get_data(train_ds, valid_ds, bs):\n","    return ( \n","            DataLoader(train_ds, batch_size=bs, shuffle=True),\n","            DataLoader(valid_ds, batch_size=bs *2),\n","            \n","            )        "]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["0 0.32619796608686447\n","1 0.29963203477859496\n"]}],"source":["train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n","model, opt = get_model()\n","fit(epochs, model, loss_func, opt, train_dl, valid_dl)"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["# CNN\n","\n","class Mnist_CNN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1)\n","        self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1)\n","        self.conv3 = nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1)\n","        \n","    def forward(self, xb):\n","        xb = xb.view(-1, 1, 28, 28)\n","        xb = F.relu(self.conv1(xb))\n","        xb = F.relu(self.conv2(xb))\n","        xb = F.relu(self.conv3(xb))\n","        xb = F.avg_pool2d(xb, 4)\n","        return xb.view(-1, xb.size(1))\n","                       \n","lr = 0.1                                  "]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["0 0.38569582710266115\n","1 0.2759266707897186\n"]}],"source":["model = Mnist_CNN()\n","opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n","\n","fit(epochs, model, loss_func, opt, train_dl, valid_dl)"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":["## nn.sequential\n","\n","class Lambda(nn.Module):\n","    def __init__(self, func):\n","        super().__init__()\n","        self.func = func\n","        \n","    def forward(self, x):    \n","        return self.func(x)\n","    \n","def preprocess(x):    \n","    return x.view(-1, 1, 28, 28)"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["0 1.02289142036438\n","1 0.8372313705444336\n"]}],"source":["model = nn.Sequential(\n","    Lambda(preprocess),\n","    nn.Conv2d(1,16, kernel_size=3, stride=2, padding=1),\n","    nn.ReLU(),\n","    nn.Conv2d(16,16, kernel_size=3, stride=2, padding=1),\n","    nn.ReLU(),\n","    nn.Conv2d(16,10, kernel_size=3, stride=2, padding=1),\n","    nn.ReLU(),\n","    nn.AvgPool2d(4),\n","    Lambda(lambda x: x.view(x.size(0), -1)),\n",")\n","\n","opt = optim.SGD(model.parameters(),lr=lr, momentum=0.9)\n","\n","fit(epochs, model, loss_func, opt, train_dl, valid_dl)"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[],"source":["def preprocess(x, y):\n","    return x.view(-1, 1, 28, 28), y\n","\n","class WrappedDataLoader:\n","    def __init__(self, dl, func):\n","        self.dl = dl\n","        self.func = func\n","        \n","    def __len__(self):\n","        return len(self.dl)\n","    \n","    def __iter__(self):\n","        batches = iter(self.dl)\n","        for b in batches:\n","            yield (self.func(*b))\n","            \n","            \n","train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n","train_dl = WrappedDataLoader(train_dl, preprocess)\n","valid_dl = WrappedDataLoader(valid_dl, preprocess)            \n","            \n","                \n","         "]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[],"source":["model = nn.Sequential(\n","    \n","    nn.Conv2d(1,16, kernel_size=3, stride=2, padding=1),\n","    nn.ReLU(),\n","    nn.Conv2d(16,16, kernel_size=3, stride=2, padding=1),\n","    nn.ReLU(),\n","    nn.Conv2d(16,10, kernel_size=3, stride=2, padding=1),\n","    nn.ReLU(),\n","    nn.AdaptiveAvgPool2d(1),\n","    Lambda(lambda x: x.view(x.size(0), -1)),\n",")\n","opt = optim.SGD(model.parameters(),lr=lr, momentum=0.9)"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["0 0.3475101107358933\n","1 0.23465721834301947\n"]}],"source":["fit(epochs, model, loss_func, opt, train_dl, valid_dl)"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["False\n"]}],"source":["print(torch.cuda.is_available())"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[],"source":["dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",""]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[],"source":["def preprocess(x, y):\n","    return x.view(-1, 1, 28, 28).to(dev), y.to(dev)\n","\n","train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n","train_dl = WrappedDataLoader(train_dl, preprocess)\n","valid_dl = WrappedDataLoader(valid_dl, preprocess)"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[],"source":["model.to(dev)\n","opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["0 0.2861466431736946\n","1 0.19988121993988753\n"]}],"source":["fit(epochs, model, loss_func, opt, train_dl, valid_dl)\n",""]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":2}}